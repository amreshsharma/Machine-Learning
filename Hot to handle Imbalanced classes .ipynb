{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to handle Imbalanced classes in Machine Learning\n",
    "\n",
    "##### What is imbalance data/class\n",
    "\n",
    "Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally.\n",
    "\n",
    "For example, you may have a 2-class (binary) classification problem with 100 instances (rows). A total of 80 instances are labeled with Class-1 and the remaining 20 instances are labeled with Class-2.\n",
    "\n",
    "This is an imbalanced dataset and the ratio of Class-1 to Class-2 instances is 4:1.\n",
    "\n",
    "We now understand what class imbalance is and why it provides misleading classification accuracy.\n",
    "\n",
    "So what are our some options?\n",
    "\n",
    "#### Can You Collect More Data?\n",
    "\n",
    "You might think it’s silly, but collecting more data is almost always overlooked.\n",
    "Can you collect more data? Take a second and think about whether you are able to gather more data on your problem.\n",
    "A larger dataset might expose a different and perhaps more balanced perspective on the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this educational turorial, we’ll use a synthetic dataset called Balance Scale Data, which you can download from the UCI Machine Learning Repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balance</th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>var4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  balance  var1  var2  var3  var4\n",
       "0       B     1     1     1     1\n",
       "1       R     1     1     1     2\n",
       "2       R     1     1     1     3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('balance-scale.data',names=['balance', 'var1', 'var2', 'var3', 'var4'])\n",
    "df.head(3)         # first 10 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has:\n",
    "1 target variable, which we've named balance .\n",
    "4 input features, which we've named var1  through var4 .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R    288\n",
       "L    288\n",
       "B     49\n",
       "Name: balance, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['balance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 49 number of balance class of total 625\n",
    "\n",
    "We're going to label each observation as 1 (positive class) if the scale is balanced or 0 (negative class) if the scale is not balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    576\n",
       "1     49\n",
       "Name: balance, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform into binary classification\n",
    "df['balance'] = [1 if b=='B' else 0 for b in df.balance]\n",
    " \n",
    "df['balance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Danger of Imbalanced Classes\n",
    "\n",
    "Now that we have a dataset, we can really show the dangers of imbalanced classes.\n",
    "\n",
    "First, let's import the Logistic Regression algorithm and the accuracy metric from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Separate input features (X) and target variable (y)\n",
    "y = df.balance\n",
    "X = df.drop('balance', axis=1)\n",
    " \n",
    "# Train model\n",
    "clf_0 = LogisticRegression().fit(X, y)\n",
    " \n",
    "# Predict on training set\n",
    "pred_y_0 = clf_0.predict(X)\n",
    "\n",
    "# model accuracy\n",
    "\n",
    "print( accuracy_score(pred_y_0, y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model got 92% accuracy because it's predicting only 1 class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print( np.unique( pred_y_0 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this model is only predicting 0, which means it's completely ignoring the minority class in favor of the majority class.\n",
    "\n",
    "we will check 4 tactics for handling imbalanced classes in machine learning:\n",
    "\n",
    "1. Up-sample the minority class\n",
    "2. Down-sample the majority class\n",
    "3. Change your performance metric\n",
    "4. Penalize algorithms (cost-sensitive training)\n",
    "    \n",
    "Next, we'll look at the first technique for handling imbalanced classes: up-sampling the minority class.\n",
    "\n",
    "#### 1. Up-sample Minority Class\n",
    "\n",
    "Up-sampling is the process of randomly duplicating observations from the minority class in order to reinforce its signal.\n",
    "\n",
    "There are several heuristics for doing so, but the most common way is to simply resample with replacement.\n",
    "\n",
    "import the resampling module from Scikit-Learn:\n",
    "\n",
    "Next, we'll create a new DataFrame with an up-sampled minority class. Here are the steps:\n",
    "\n",
    "1. First, we'll separate observations from each class into different DataFrames.\n",
    "2. Next, we'll resample the minority class with replacement, setting the number of samples to match\n",
    "   that of the majority class.\n",
    "3. Finally, we'll combine the up-sampled minority class DataFrame with the original majority class DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    576\n",
       "0    576\n",
       "Name: balance, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = df[df.balance==0]\n",
    "df_minority = df[df.balance==1]\n",
    "\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=576,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.balance.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the new DataFrame has more observations than the original, and the ratio of the two classes is now 1:1.\n",
    "\n",
    "Let's train another model using Logistic Regression, this time on the balanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "0.513888888889\n"
     ]
    }
   ],
   "source": [
    "# Separate input features (X) and target variable (y)\n",
    "y = df_upsampled.balance\n",
    "X = df_upsampled.drop('balance', axis=1)\n",
    " \n",
    "# Train model\n",
    "clf_1 = LogisticRegression().fit(X, y)\n",
    " \n",
    "# Predict on training set\n",
    "pred_y_1 = clf_1.predict(X)\n",
    " \n",
    "# Is our model still predicting just one class?\n",
    "print( np.unique( pred_y_1 ) )\n",
    "\n",
    " \n",
    "# model accuracy\n",
    "print( accuracy_score(y, pred_y_1) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from result it is clear it is predictiong 2 class and accuracy is 51%. \n",
    "\n",
    "#### 2. Down-sample Majority Class\n",
    "\n",
    "Down-sampling involves randomly removing observations from the majority class to prevent its signal from dominating the learning algorithm.\n",
    "\n",
    "The most common heuristic for doing so is resampling without replacement.\n",
    "\n",
    "The process is similar to that of up-sampling. Here are the steps:\n",
    "\n",
    "1. First, we'll separate observations from each class into different DataFrames.\n",
    "2. Next, we'll resample the majority class without replacement, setting the number of samples to match\n",
    "    that of the minority class.\n",
    "3. Finally, we'll combine the down-sampled majority class DataFrame with the original minority class DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    49\n",
       "0    49\n",
       "Name: balance, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = df[df.balance==0]\n",
    "df_minority = df[df.balance==1]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=49,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    " \n",
    "# Display new class counts\n",
    "df_downsampled.balance.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the new DataFrame has fewer observations than the original, and the ratio of the two classes is now 1:1.\n",
    "\n",
    "Again, let's train a model using Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "0.581632653061\n"
     ]
    }
   ],
   "source": [
    "y = df_downsampled.balance\n",
    "X = df_downsampled.drop('balance', axis=1)\n",
    " \n",
    "# Train model\n",
    "clf_2 = LogisticRegression().fit(X, y)\n",
    " \n",
    "# Predict on training set\n",
    "pred_y_2 = clf_2.predict(X)\n",
    " \n",
    "# number of class?\n",
    "print( np.unique( pred_y_2 ) )\n",
    "\n",
    " \n",
    "# model accuracy?\n",
    "print( accuracy_score(y, pred_y_2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 3. Change our Performance Metric\n",
    "\n",
    "we've looked at two ways of addressing imbalanced classes by resampling the dataset. Next, we'll look at using other performance metrics for evaluating the models.\n",
    "\n",
    "For a general-purpose metric for classification, we recommend Area Under ROC Curve\n",
    "\n",
    "To calculate AUROC, you'll need predicted class probabilities instead of just the predicted classes. You can get them using the .predict_proba()  function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45419197226479618, 0.48205962213283882, 0.46862327066392478, 0.47868378832689107, 0.58143856820159667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict class probabilities\n",
    "prob_y_2 = clf_2.predict_proba(X)\n",
    " \n",
    "# Keep only the positive class\n",
    "prob_y_2 = [p[1] for p in prob_y_2]\n",
    " \n",
    "print( prob_y_2[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.568096626406\n"
     ]
    }
   ],
   "source": [
    "print( roc_auc_score(y, prob_y_2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.476468138276\n"
     ]
    }
   ],
   "source": [
    "# and how does this compare to the original model trained on the imbalanced dataset?\n",
    "\n",
    "prob_y_0 = clf_0.predict_proba(X)\n",
    "prob_y_0 = [p[1] for p in prob_y_0]\n",
    " \n",
    "print( roc_auc_score(y, prob_y_0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Penalize Algorithms (Cost-Sensitive Training)\n",
    "\n",
    "The next tactic is to use penalized learning algorithms that increase the cost of classification mistakes on the minority class.\n",
    "\n",
    "A popular algorithm for this technique is Penalized-SVM:\n",
    "\n",
    "During training, we can use the argument class_weight='balanced'  to penalize mistakes on the minority class by an amount proportional to how under-represented it is.\n",
    "\n",
    "We also want to include the argument probability=True  if we want to enable probability estimates for SVM algorithms.\n",
    "\n",
    "Let's train a model using Penalized-SVM on the original imbalanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "0.688\n",
      "0.4694763322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "y = df.balance\n",
    "X = df.drop('balance', axis=1)\n",
    " \n",
    "# Train model\n",
    "clf_3 = SVC(kernel='linear', \n",
    "            class_weight='balanced', # penalize\n",
    "            probability=True)\n",
    " \n",
    "clf_3.fit(X, y)\n",
    " \n",
    "# Predict on training set\n",
    "pred_y_3 = clf_3.predict(X)\n",
    " \n",
    "# number of class\n",
    "print( np.unique( pred_y_3 ) )\n",
    "\n",
    " \n",
    "# model accuracy result\n",
    "print( accuracy_score(y, pred_y_3) )\n",
    "\n",
    " \n",
    "# compare with AUROC\n",
    "prob_y_3 = clf_3.predict_proba(X)\n",
    "prob_y_3 = [p[1] for p in prob_y_3]\n",
    "\n",
    "print( roc_auc_score(y, prob_y_3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
